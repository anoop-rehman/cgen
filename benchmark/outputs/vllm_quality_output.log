
==========
== CUDA ==
==========

CUDA Version 12.4.1

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

/usr/local/lib/python3.10/dist-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
No module named 'vllm._version'
  from vllm.version import __version__ as VLLM_VERSION
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 09-10 18:31:06 config.py:887] Defaulting to use mp for distributed inference
INFO 09-10 18:31:06 config.py:1005] Chunked prefill is enabled with max_num_batched_tokens=2048.
WARNING 09-10 18:31:06 config.py:357] Async output processing can not be enabled with pipeline parallel
INFO 09-10 18:31:06 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='meta-llama/Llama-2-7b-hf', speculative_config=None, tokenizer='meta-llama/Llama-2-7b-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=2, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-2-7b-hf, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=True multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)
WARNING 09-10 18:31:06 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 09-10 18:31:06 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[1;36m(VllmWorkerProcess pid=398)[0;0m INFO 09-10 18:31:06 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=400)[0;0m INFO 09-10 18:31:06 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=399)[0;0m INFO 09-10 18:31:06 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
INFO 09-10 18:31:07 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=399)[0;0m INFO 09-10 18:31:07 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=398)[0;0m INFO 09-10 18:31:07 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=400)[0;0m INFO 09-10 18:31:07 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=399)[0;0m INFO 09-10 18:31:07 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 09-10 18:31:07 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=398)[0;0m INFO 09-10 18:31:07 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=400)[0;0m INFO 09-10 18:31:07 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 09-10 18:31:08 custom_all_reduce_utils.py:204] generating GPU P2P access cache in /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
INFO 09-10 18:31:21 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorkerProcess pid=398)[0;0m INFO 09-10 18:31:21 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorkerProcess pid=400)[0;0m INFO 09-10 18:31:21 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorkerProcess pid=399)[0;0m INFO 09-10 18:31:21 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
WARNING 09-10 18:31:21 custom_all_reduce.py:141] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=400)[0;0m WARNING 09-10 18:31:21 custom_all_reduce.py:141] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=399)[0;0m WARNING 09-10 18:31:21 custom_all_reduce.py:141] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=398)[0;0m WARNING 09-10 18:31:21 custom_all_reduce.py:141] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-10 18:31:21 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7900aea7e920>, local_subscribe_port=36669, remote_subscribe_port=None)
[1;36m(VllmWorkerProcess pid=399)[0;0m INFO 09-10 18:31:21 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7900ae869960>, local_subscribe_port=37241, remote_subscribe_port=None)
[1;36m(VllmWorkerProcess pid=399)[0;0m INFO 09-10 18:31:21 utils.py:1008] Found nccl from library libnccl.so.2
INFO 09-10 18:31:21 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=399)[0;0m INFO 09-10 18:31:21 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 09-10 18:31:21 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=400)[0;0m INFO 09-10 18:31:21 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=398)[0;0m INFO 09-10 18:31:21 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=398)[0;0m INFO 09-10 18:31:21 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=400)[0;0m INFO 09-10 18:31:21 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 09-10 18:31:21 model_runner.py:1060] Starting to load model meta-llama/Llama-2-7b-hf...
[1;36m(VllmWorkerProcess pid=399)[0;0m INFO 09-10 18:31:21 model_runner.py:1060] Starting to load model meta-llama/Llama-2-7b-hf...
[1;36m(VllmWorkerProcess pid=400)[0;0m INFO 09-10 18:31:21 model_runner.py:1060] Starting to load model meta-llama/Llama-2-7b-hf...
[1;36m(VllmWorkerProcess pid=398)[0;0m INFO 09-10 18:31:21 model_runner.py:1060] Starting to load model meta-llama/Llama-2-7b-hf...
INFO 09-10 18:31:22 weight_utils.py:243] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=398)[0;0m INFO 09-10 18:31:22 weight_utils.py:243] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=400)[0;0m INFO 09-10 18:31:22 weight_utils.py:243] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=399)[0;0m INFO 09-10 18:31:22 weight_utils.py:243] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.07it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  2.24it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.93it/s]

INFO 09-10 18:31:23 model_runner.py:1071] Loading model weights took 3.1555 GB
[1;36m(VllmWorkerProcess pid=400)[0;0m INFO 09-10 18:31:23 model_runner.py:1071] Loading model weights took 3.1555 GB
[1;36m(VllmWorkerProcess pid=398)[0;0m INFO 09-10 18:31:23 model_runner.py:1071] Loading model weights took 3.1555 GB
[1;36m(VllmWorkerProcess pid=399)[0;0m INFO 09-10 18:31:23 model_runner.py:1071] Loading model weights took 3.1555 GB
INFO 09-10 18:31:24 distributed_gpu_executor.py:57] # GPU blocks: 8241, # CPU blocks: 2048
INFO 09-10 18:31:24 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 32.19x
[1;36m(VllmWorkerProcess pid=398)[0;0m INFO 09-10 18:31:26 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=398)[0;0m INFO 09-10 18:31:26 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 09-10 18:31:26 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 09-10 18:31:26 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=399)[0;0m INFO 09-10 18:31:26 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=399)[0;0m INFO 09-10 18:31:26 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=400)[0;0m INFO 09-10 18:31:26 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=400)[0;0m INFO 09-10 18:31:26 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=399)[0;0m INFO 09-10 18:31:49 model_runner.py:1530] Graph capturing finished in 23 secs.
[1;36m(VllmWorkerProcess pid=400)[0;0m INFO 09-10 18:31:49 model_runner.py:1530] Graph capturing finished in 23 secs.
INFO 09-10 18:31:50 model_runner.py:1530] Graph capturing finished in 23 secs.
[1;36m(VllmWorkerProcess pid=398)[0;0m INFO 09-10 18:31:50 model_runner.py:1530] Graph capturing finished in 24 secs.
Running vLLM benchmark with 10 requests...
Model: meta-llama/Llama-2-7b-hf
Tensor Parallel: 2, Pipeline Parallel: 2
Chunked Prefill: True, Max Batched Tokens: 2048
Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][rank0]: Traceback (most recent call last):
[rank0]:   File "/workspace/cgen/benchmark/benchmark_vllm.py", line 178, in <module>
[rank0]:     launch(args)
[rank0]:   File "/workspace/cgen/benchmark/benchmark_vllm.py", line 129, in launch
[rank0]:     dur = run_vllm(
[rank0]:   File "/workspace/cgen/benchmark/benchmark_vllm.py", line 69, in run_vllm
[rank0]:     outputs = llm.generate(inputs, sampling_params)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/vllm/utils.py", line 1063, in inner
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/llm.py", line 353, in generate
[rank0]:     outputs = self._run_engine(use_tqdm=use_tqdm)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/llm.py", line 879, in _run_engine
[rank0]:     step_outputs = self.llm_engine.step()
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py", line 1312, in step
[rank0]:     raise NotImplementedError(
[rank0]: NotImplementedError: Pipeline parallelism is only supported through AsyncLLMEngine as performance will be severely degraded otherwise.
/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
INFO 09-10 18:31:52 multiproc_worker_utils.py:121] Killing local vLLM worker processes
Processed prompts:   0%|          | 0/10 [00:02<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
