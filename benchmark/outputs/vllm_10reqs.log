ubuntu@ip-172-31-35-196:~/workspace/seesaw/cgen$ DATASET=arxiv MODEL=elinas/Llama-3-15B-Instruct-zeroed TP=4 PP=1 CHUNKED_PREFILL=1 MAX_BATCHED_TOKENS=2048 SORT_PROMPTS=1 ENFORCE_EAGER=1 NUM_REQUESTS=10 INPUT_LEN=1000 OUTPUT_LEN
=100 CSV=runs_4xl4.csv CSV_APPEND=1 PRINT_OUTPUT=1 benchmark/run_vllm_async.sh

==========
== CUDA ==
==========

CUDA Version 12.4.1

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

/usr/local/lib/python3.10/dist-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
No module named 'vllm._version'
  from vllm.version import __version__ as VLLM_VERSION
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 09-12 23:24:06 config.py:1005] Chunked prefill is enabled with max_num_batched_tokens=2048.
WARNING 09-12 23:24:06 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 09-12 23:24:06 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='elinas/Llama-3-15B-Instruct-zeroed', speculative_config=None, tokenizer='elinas/Llama-3-15B-Instruct-zeroed', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=elinas/Llama-3-15B-Instruct-zeroed, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=True multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)
WARNING 09-12 23:24:06 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 09-12 23:24:06 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
(VllmWorkerProcess pid=303) INFO 09-12 23:24:07 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
(VllmWorkerProcess pid=304) INFO 09-12 23:24:07 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
(VllmWorkerProcess pid=305) INFO 09-12 23:24:07 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
(VllmWorkerProcess pid=303) INFO 09-12 23:24:08 utils.py:1008] Found nccl from library libnccl.so.2
INFO 09-12 23:24:08 utils.py:1008] Found nccl from library libnccl.so.2
(VllmWorkerProcess pid=304) INFO 09-12 23:24:08 utils.py:1008] Found nccl from library libnccl.so.2
(VllmWorkerProcess pid=303) INFO 09-12 23:24:08 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 09-12 23:24:08 pynccl.py:63] vLLM is using nccl==2.20.5
(VllmWorkerProcess pid=304) INFO 09-12 23:24:08 pynccl.py:63] vLLM is using nccl==2.20.5
(VllmWorkerProcess pid=305) INFO 09-12 23:24:08 utils.py:1008] Found nccl from library libnccl.so.2
(VllmWorkerProcess pid=305) INFO 09-12 23:24:08 pynccl.py:63] vLLM is using nccl==2.20.5
(VllmWorkerProcess pid=304) WARNING 09-12 23:24:09 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
(VllmWorkerProcess pid=303) WARNING 09-12 23:24:09 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
(VllmWorkerProcess pid=305) WARNING 09-12 23:24:09 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 09-12 23:24:09 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 09-12 23:24:09 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7776ac9e3f10>, local_subscribe_port=52199, remote_subscribe_port=None)
INFO 09-12 23:24:09 model_runner.py:1060] Starting to load model elinas/Llama-3-15B-Instruct-zeroed...
(VllmWorkerProcess pid=303) INFO 09-12 23:24:09 model_runner.py:1060] Starting to load model elinas/Llama-3-15B-Instruct-zeroed...
(VllmWorkerProcess pid=304) INFO 09-12 23:24:09 model_runner.py:1060] Starting to load model elinas/Llama-3-15B-Instruct-zeroed...
(VllmWorkerProcess pid=305) INFO 09-12 23:24:09 model_runner.py:1060] Starting to load model elinas/Llama-3-15B-Instruct-zeroed...
INFO 09-12 23:24:09 weight_utils.py:243] Using model weights format ['*.safetensors']
(VllmWorkerProcess pid=303) INFO 09-12 23:24:09 weight_utils.py:243] Using model weights format ['*.safetensors']
(VllmWorkerProcess pid=305) INFO 09-12 23:24:09 weight_utils.py:243] Using model weights format ['*.safetensors']
(VllmWorkerProcess pid=304) INFO 09-12 23:24:09 weight_utils.py:243] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.79it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.70it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.61it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.37it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.06it/s]

(VllmWorkerProcess pid=303) INFO 09-12 23:24:11 model_runner.py:1071] Loading model weights took 6.9922 GB
INFO 09-12 23:24:11 model_runner.py:1071] Loading model weights took 6.9922 GB
(VllmWorkerProcess pid=304) INFO 09-12 23:24:11 model_runner.py:1071] Loading model weights took 6.9922 GB
(VllmWorkerProcess pid=305) INFO 09-12 23:24:11 model_runner.py:1071] Loading model weights took 6.9922 GB
INFO 09-12 23:24:13 distributed_gpu_executor.py:57] # GPU blocks: 11644, # CPU blocks: 4096
INFO 09-12 23:24:13 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 45.48x
Running vLLM (Async) with 10 requests...
Model: elinas/Llama-3-15B-Instruct-zeroed
TP=4, PP=1
Chunked Prefill: True, Max Batched Tokens: 2048
NOTE: enforce_eager=True (CUDA graphs disabled)
INFO 09-12 23:24:16 async_llm_engine.py:209] Added request req-0.
INFO 09-12 23:24:16 async_llm_engine.py:209] Added request req-1.
INFO 09-12 23:24:16 async_llm_engine.py:209] Added request req-2.
INFO 09-12 23:24:16 async_llm_engine.py:209] Added request req-3.
INFO 09-12 23:24:16 async_llm_engine.py:209] Added request req-4.
INFO 09-12 23:24:16 async_llm_engine.py:209] Added request req-5.
INFO 09-12 23:24:16 async_llm_engine.py:209] Added request req-6.
INFO 09-12 23:24:16 async_llm_engine.py:209] Added request req-7.
INFO 09-12 23:24:16 async_llm_engine.py:209] Added request req-8.
INFO 09-12 23:24:16 async_llm_engine.py:209] Added request req-9.
INFO 09-12 23:24:21 metrics.py:345] Avg prompt throughput: 2203.6 tokens/s, Avg generation throughput: 4.5 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 3 reqs, GPU KV cache usage: 7.3%, CPU KV cache usage: 0.0%.
INFO 09-12 23:24:27 metrics.py:345] Avg prompt throughput: 2382.7 tokens/s, Avg generation throughput: 9.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.1%, CPU KV cache usage: 0.0%.
INFO 09-12 23:24:32 metrics.py:345] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 180.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 13.6%, CPU KV cache usage: 0.0%.
INFO 09-12 23:24:32 async_llm_engine.py:177] Finished request req-8.
INFO 09-12 23:24:32 async_llm_engine.py:221] Aborted request req-8.
INFO 09-12 23:24:33 async_llm_engine.py:177] Finished request req-1.
INFO 09-12 23:24:33 async_llm_engine.py:221] Aborted request req-1.
INFO 09-12 23:24:33 async_llm_engine.py:177] Finished request req-3.
INFO 09-12 23:24:33 async_llm_engine.py:221] Aborted request req-3.
INFO 09-12 23:24:33 async_llm_engine.py:177] Finished request req-0.
INFO 09-12 23:24:33 async_llm_engine.py:221] Aborted request req-0.
INFO 09-12 23:24:34 async_llm_engine.py:177] Finished request req-5.
INFO 09-12 23:24:34 async_llm_engine.py:221] Aborted request req-5.
INFO 09-12 23:24:34 async_llm_engine.py:177] Finished request req-9.
INFO 09-12 23:24:34 async_llm_engine.py:221] Aborted request req-9.
INFO 09-12 23:24:35 async_llm_engine.py:177] Finished request req-7.
INFO 09-12 23:24:35 async_llm_engine.py:221] Aborted request req-7.
INFO 09-12 23:24:37 metrics.py:345] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 117.2 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.0%, CPU KV cache usage: 0.0%.
INFO 09-12 23:24:41 async_llm_engine.py:177] Finished request req-6.
INFO 09-12 23:24:41 async_llm_engine.py:221] Aborted request req-6.
INFO 09-12 23:24:42 metrics.py:345] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 61.3 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 09-12 23:24:43 async_llm_engine.py:177] Finished request req-4.
INFO 09-12 23:24:43 async_llm_engine.py:221] Aborted request req-4.
INFO 09-12 23:24:47 metrics.py:345] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 25.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 09-12 23:24:52 metrics.py:345] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 21.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.
INFO 09-12 23:24:57 metrics.py:345] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 22.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.
INFO 09-12 23:25:02 metrics.py:345] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 21.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.
INFO 09-12 23:25:07 metrics.py:345] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 21.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.
INFO 09-12 23:25:11 async_llm_engine.py:177] Finished request req-2.
=== Sample vLLM Outputs (First 2) ===

Sample 1:
Input: for fixed integers @xmath0 and @xmath1 , we consider the admissible sequences of @xmath2 lattice paths in a colored @xmath3 square given in @xcite . each admissible sequence of paths can be associated with a partition @xmath10 of @xmath4 . in section 
 [ paths ] , we show that the number of self - conjugate admissible sequences of paths associated with @xmath10 is equal to the number of standard young tableaux of shape @xmath10 , and thus can be calculated using the hook length formula . 
 we extend this result to include the non - self - conjugate admissible sequences of paths and show that the number of all such admissible sequences of paths is equal to the sum of squares of the number of standard young tableaux of partitions of @xmath4 with height less than or equal to @xmath11 . using the rsk correspondence in @xcite , 
 it is shown in ( @xcite , corollary 7.23.12 ) that the sum of squares of the number of standard young tableaux of partitions of @xmath4 with height less than or equal to @xmath11 is equal to the number of @xmath6-avoiding permutations of @xmath7 .    in section [ multiplicities ] 
 , we apply our results to the representation theory of the affine kac - moody algebra @xmath8 . 
 let @xmath12 , @xmath13 and @xmath14 denote the simple roots , simple coroots , and fundamental weights respectively . 
 note that @xmath15 
 . for @xmath16 , set @xmath17 and @xmath18 . as shown in @xcite , @xmath19 
 are maximal dominant weights of the irreducible @xmath8-module @xmath9 . 
 we show that the multiplicity of the weight @xmath19 in @xmath9 is the number of @xmath6-avoiding permutations of @xmath7 , which proves conjecture 4.13 in @xcite . 
 for fixed integers @xmath0 and @xmath1 , consider the @xmath3 square containing @xmath20 unit boxes in the fourth quadrant so that the top left corner of the square is at the origin . 
 we assign color @xmath21 to a box if its upper left corner has coordinates @xmath22 . 
 this gives the following @xmath3 colored square @xmath23 :      a lattice path @xmath25 on @xmath23 is a path joining the lower left corner @xmath26 to the upper right corner @xmath27 moving unit lengths up or right . for two lattice paths @xmath28 on @xmath23 
 we say that @xmath29 if the boxes above @xmath30 are also above @xmath25 . 
 now , we draw @xmath2 lattice paths , @xmath31 on @xmath23 such that @xmath32 . for integers 
 @xmath33 , where @xmath34 , @xmath35 , we define @xmath36 to be the number of @xmath37-colored boxes between @xmath38 and @xmath39 . 
 we define @xmath40 to be the number of @xmath37-colored boxes below @xmath41 and @xmath42 to be the number of @xmath37-colored boxes above @xmath43 . 
 denote by @xmath49 the set of all admissible sequences of @xmath2 paths . 
 notice that there are @xmath4 0-colored boxes in @xmath23 and hence for any admissible sequence of paths , @xmath50 . 
 in addition , it follows from definition [ pathsdef](2 ) that @xmath51 for any admissible sequence of paths . 
 thus , we can and do associate an admissible sequence of paths @xmath44 on @xmath23 with a partition @xmath52 of @xmath4 . in this case , we say that this admissible sequence of paths is of type @xmath10 and often draw @xmath10 as a young diagram . 
 figure [ adseq](a ) is an element of @xmath53 , where @xmath54 and @xmath55 are shown in figures [ adseq](b ) , [ adseq](c ) , and [ adseq](d ) , respectively . 
 notice that this admissible sequence of paths is of type @xmath56 .
Generated: 
 we can also associate a partition @xmath57 of @xmath4 with an admissible sequence of paths @xmath44 on @xmath23 . 
 in this case , we say that this admissible sequence of paths is of type @xmath10 and often draw @xmath10 as a young diagram . 
 figure [ adseq](a ) is an element of @xmath58 , where @xmath59 and @xmath60 are shown in figures [ adseq](b ) , [ adseq](c ) , and [ adseq](d ) , respectively . 
 notice that this admissible sequence of paths is of type @xmath61 . 
 we can
--------------------------------------------------------------------------------

Sample 2:
Input: there are reasons to believe that cosmic rays ( crs ) around the ankle at @xmath0 gev are dominated by extragalactic protons  @xcite . 
 scattering processes in the cosmic microwave background ( cmb ) limit the propagation of ultra high energy ( uhe ) charged particles in our universe . 
 a continuation of a power - like cr spectrum above the greisen - zatsepin - kuzmin ( gzk ) cutoff  @xcite at about @xmath1 gev is only consistent with the proton dominance if the sources lie within the proton attenuation length of about 50 mpc . 
 very few astrophysical accelerators can generate crs with energies above the gzk cutoff  ( see e.g.  @xcite for a review ) and so far none of the candidate sources have been confirmed in our local environment . 
 it has been speculated that decaying superheavy particles , possibly some new form of dark matter or remnants of topological defects , could be a source of uhe crs , but also these proposals are not fully consistent with the cr spectrum at lower energies  @xcite . 
 the observation of gzk excesses has led to speculations about a different origin of uhe crs . 
 berezinsky and zatsepin  @xcite proposed that _ 
 cosmogenic _ neutrinos produced in the decay of the gzk photopions could explain these events assuming a strong neutrino nucleon interaction . 
 we have followed this idea in ref . 
 @xcite and investigated the statistical goodness of scenarios with strongly interacting neutrinos from optically thin sources using cr data from agasa  @xcite and hires  @xcite ( see fig .  [ cr ] ) and limits from horizontal events at agasa  @xcite and contained events at rice  @xcite . 
 -branes , and string excitations ( see ref . 
 @xcite ) . ] 
 the flux of uhe extragalactic protons from distant sources is redshifted and also subject to @xmath2 pair production and photopion - production in the cmb which can be taken into account by means of propagation functions . 
 the resonantly produced photopions provide a _ guaranteed _ source of cosmogenic uhe neutrinos observed at earth . in astrophysical accelerators inelastic scattering of the beam protons off the ambient photon gas in the source will also produce photopions which provide an additional source of uhe neutrinos . the corresponding spectrum will in general depend on the details of the source such as the densities of the target photons and the ambient gas  @xcite . 
 we have used the flux of crs from _ optically thin _ sources using the luminosities given in ref . 
 @xcite in the goodness - of - fit test .    for a reasonable and consistent contribution of extragalactic neutrinos in vertical crs 
 one has to assume a strong and rapid enhancement of the neutrino nucleon interaction . 
 the realization of such a behavior has been proposed in scenarios beyond the ( perturbative ) sm ( see ref . 
 @xcite ) . for convenience 
 , we have approximated the strong neutrino nucleon cross section in our analysis by a @xmath3-behavior shown in fig . 
 [ fig ] , parameterized by the energy scale and width of the transition , and the amplification compared to the standard model predictions . 
 our analysis showed that uhe crs measured at agasa and hires can be interpreted to the 90% cl as a composition of extragalactic protons and strongly interacting neutrinos from optically thin sources in agreement with experimental results from horizontal events at agasa and contained events at rice ( see fig . 
 [ fig ] ) . 
 the pierre auger observatory combines the experimental techniques of agasa and hires as a hybrid detector . with a better energy resolution , much higher statistics and also stronger bounds on horizontal showers 
 it will certainly help to clarify our picture of uhe crs in the future . 
 the author would like to thank the organizers of the erice school on nuclear physics 2005 _ `` neutrinos in cosmology , in astro , particle and nuclear physic '' _ for the inspiring workshop and vihkos ( _ `` virtuelles institut fr hochenergiestrahlungen aus dem kosmos '' _ ) for support . 
 m.  ahlers , a.  ringwald , and h.  tu , _ astropart . 
 ( to appear ) , preprint astro - ph/0506698 . 
 v.  berezinsky , a.  z.  gazizov and s.  i.  grigorieva , preprint hep - ph/0204357 ; v.  berezinsky , a.  z.  gazizov and s.  i.  grigorieva , . 
 m.  ahlers _ 
 et  al . _ , . 
 k.  greisen , ; g.  t.  zatsepin and v.  a.  kuzmin , . 
 d.  f.  torres and l.  a.  anchordoqui , . 
 d.  v.  semikoz and g.  sigl , . 
 v.  s.  beresinsky and g.  t.  zatsepin , . 
 m.  takeda _ 
 et  al . _ [ agasa ] , . 
 d.  j.  bird _ 
 et  al . _ 
 [ hires ] , ; r.  u.  abbasi _ et  al . _ [ hires ] , ; r.  u.  abbasi _ et  al . _ [ hires ] , . s.  yoshida _ 
 _ [ agasa ] , .
Generated: 
 m.  ahlers _ 
 et  al . _ , . 
 m.  ahlers _ 
 et  al . _ , . 
 m.  ahlers _ 
 et  al . _ , . 
 m.  ahlers _ 
 et  al . _ , . 
 m.  ahlers _ 
 et  al . _ , . 
 m.  ahlers _ 
 et  al . _ , . 
 m.  ahlers _ 
 et  al . _ , . 
 m.  ahlers _ 
 et  al . _ , . 
 m.  ahlers
--------------------------------------------------------------------------------
=== Token accounting (actual) ===
Actual prefilled tokens: 24349
Actual generated tokens: 2549

=== Throughput (actual / wall time) ===
Prefill throughput: 442.15 tokens/s
Decode  throughput: 46.29 tokens/s
Total   throughput: 488.43 tokens/s
Wall time: 55.07s
INFO 09-12 23:25:11 async_llm_engine.py:63] Engine is gracefully shutting down.
Exception in callback Future.set_result(None)
handle: <Handle Future.set_result(None)>
Traceback (most recent call last):
  File "/usr/lib/python3.10/asyncio/events.py", line 80, in _run
    self._context.run(self._callback, *self._args)
asyncio.exceptions.InvalidStateError: invalid state
Exception in callback Future.set_result(None)
handle: <Handle Future.set_result(None)>
Traceback (most recent call last):
  File "/usr/lib/python3.10/asyncio/events.py", line 80, in _run
    self._context.run(self._callback, *self._args)
asyncio.exceptions.InvalidStateError: invalid state
Exception in callback Future.set_result(None)
handle: <Handle Future.set_result(None)>
Traceback (most recent call last):
  File "/usr/lib/python3.10/asyncio/events.py", line 80, in _run
    self._context.run(self._callback, *self._args)
asyncio.exceptions.InvalidStateError: invalid state
Total time: 55.07s
Wrote CSV row to: /workspace/cgen/runs_4xl4.csv
ERROR 09-12 23:25:13 multiproc_worker_utils.py:117] Worker VllmWorkerProcess pid 303 died, exit code: -15
INFO 09-12 23:25:13 multiproc_worker_utils.py:121] Killing local vLLM worker processes
/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
ubuntu@ip-172-31-35-196:~/workspace/seesaw/cgen$ 

